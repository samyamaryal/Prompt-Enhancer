{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 11:29:12.633628: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734370152.666675   62708 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734370152.676623   62708 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-16 11:29:12.751858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1a76f1a551475381b2755efd2675d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForCausalLM, AutoTokenizer \n",
    "import torch \n",
    "import nltk \n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "\n",
    "SEED = torch.manual_seed(336) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3 sentences or lesser from the generated prompt\n",
    "def get_processed_prompt(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return ' '.join(sentences[:2])\n",
    "    \n",
    "    #return text  #Testing without preprocessing the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_args = {\n",
    "    'max_length': 100,\n",
    "    'no_repeat_ngram_size': 1,\n",
    "    #'temperature': 1.3,\n",
    "    'top_p': 0.9,\n",
    "    'top_k': 100,\n",
    "    'do_sample': True\n",
    "}\n",
    "\n",
    "image_params = {'num_inference_steps':50, \n",
    "                'num_images_per_prompt':1,\n",
    "                'generator':SEED, \n",
    "                'guidance_scale':15,\n",
    "                'negative_prompt':\"animated, bad, terrible, low quality, weird\"} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_gpt_model = AutoModelForCausalLM.from_pretrained('./models/fine_tuned_gpt') \n",
    "old_gpt_tokenizer = AutoTokenizer.from_pretrained('./models/fine_tuned_gpt') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gpt_model = AutoModelForCausalLM.from_pretrained('./models/fine_tuned_gpt_new_data') \n",
    "new_gpt_tokenizer = AutoTokenizer.from_pretrained('./models/fine_tuned_gpt_new_data') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5model = T5ForConditionalGeneration.from_pretrained('./models/fine_tuned_t5') \n",
    "t5tokenizer = T5Tokenizer.from_pretrained('./models/fine_tuned_t5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate images from enhanced prompts\n",
    "def generate_images_and_prompts(input_text):\n",
    "\n",
    "\n",
    "    # Original Image\n",
    "    og_image = pipe.to(torch.device('cuda'))(input_text, **image_params).images[0]\n",
    "\n",
    "\n",
    "    #T5 image\n",
    "    # Tokenize the input text\n",
    "    t5_inputs = t5tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    t5_outputs = t5model.to(torch.device('cpu')).generate(\n",
    "        input_ids=t5_inputs['input_ids'],\n",
    "        attention_mask=t5_inputs['attention_mask'],\n",
    "        temperature=0.3,\n",
    "        **llm_args\n",
    "    )\n",
    "    t5_generated_text = get_processed_prompt(t5tokenizer.decode(t5_outputs[0], skip_special_tokens=True))\n",
    "    t5_image = pipe.to(torch.device('cuda'))(t5_generated_text, **image_params).images[0]\n",
    "\n",
    "\n",
    "    # GPT image\n",
    "    old_gpt_inputs = old_gpt_tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    outputs = old_gpt_model.to(torch.device('cpu')).generate(\n",
    "        input_ids=old_gpt_inputs['input_ids'],\n",
    "        attention_mask=old_gpt_inputs['attention_mask'],\n",
    "        temperature=0.3,\n",
    "        **llm_args\n",
    "    )\n",
    "    gpt_generated_text = get_processed_prompt(old_gpt_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    gpt_image = pipe.to(torch.device('cuda'))(gpt_generated_text, **image_params).images[0]\n",
    "\n",
    "\n",
    "    # New GPT image\n",
    "    new_gpt_inputs = new_gpt_tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    new_gpt_outputs = new_gpt_model.to(torch.device('cpu')).generate(\n",
    "        input_ids=new_gpt_inputs['input_ids'],\n",
    "        attention_mask=new_gpt_inputs['attention_mask'],\n",
    "        temperature=1.5,\n",
    "        **llm_args\n",
    "    )\n",
    "    new_gpt_generated_text = get_processed_prompt(new_gpt_tokenizer.decode(new_gpt_outputs[0], skip_special_tokens=True))\n",
    "    new_gpt_image = pipe.to(torch.device('cuda'))(new_gpt_generated_text, **image_params).images[0]\n",
    "\n",
    " \n",
    "    return input_text, og_image, t5_generated_text, t5_image, gpt_generated_text, gpt_image, new_gpt_generated_text, new_gpt_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377d8b3bb9224d76928e1490b5fbd864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313ef8a2d1714a4a8f6a2b357bcc7f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef0638227884ae4a8e75ac257b3e56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f16bc60df844df8addf3868deb257b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio Interface setup\n",
    "iface = gr.Interface(\n",
    "    fn=generate_images_and_prompts, \n",
    "    inputs=gr.Textbox(label=\"Enter your prompt for image generation\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Original Prompt\"),\n",
    "        gr.Image(type=\"pil\", label=\"Image generated from Original Prompt\"),\n",
    "\n",
    "        gr.Textbox(label=\"Prompt enhanced by T5\"),\n",
    "        gr.Image(type=\"pil\", label=\"Generated Image\"),\n",
    "\n",
    "        gr.Textbox(label=\"Prompt enhanced by GPT on old data\"),\n",
    "        gr.Image(type=\"pil\", label=\"Generated Image\"),\n",
    "\n",
    "        gr.Textbox(label=\"Prompt enhanced by GPT on new data\"),\n",
    "        gr.Image(type=\"pil\", label=\"Generated Image\"),\n",
    "\n",
    "    ],\n",
    "    live=False,\n",
    "    description=\"App\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PeacockFlock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
